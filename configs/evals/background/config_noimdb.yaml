%YAML 1.2
---
#split_kind : "query" # instances,query,...;
split_kind : "instances" # instances,query,...;
#split_kind: "instances-query"
random_split: "random" # random,custom
num_instances : 1
seed : 1
#max_set_len : 5000
#num_bins : 250

latent_variable: 0
num_latents: 128
latent_inference: 0

max_set_len : 1000
num_bins : 100

#gpu: 0
traindata_dir : "LatencyCollectorResults/new,LatencyCollectorResults/background"

#tags: "exp1_single,exp2_single,exp3_single"

#tags: "exp1,exp10_multi4_1p,exp7_multi4,exp10_multi4_1p_imdb2,exp10_multi4_1p_imdb,exp6_multi4,exp6_multi4b"

## main run
# exp1: gaming,audio,etc bk; exp6_multi4: multi6 bk;
tags: "exp1,exp6_multi4"

#tags: "exp6_multi4"
#tags: "exp1"

## only JOB
#tags: "exp10_multi4_1p_imdb2"
#tags: "exp10_multi4_1p_imdb"
#tags: "exp6_multi4b"

## has some bad files, and only JOB queries
#tags: "exp7_multi4"

#tags: "exp1_single,exp2_single,exp3_single,exp1,exp10_multi4_1p,exp7_multi4,exp10_multi4_1p_imdb2,exp10_multi4_1p_imdb,exp6_multi4,exp6_multi4b,exp4_ceb2,exp7_single_ceb2"

use_eval_tags : 0
eval_dirs : "LatencyCollectorResults/new"
eval_tags : "exp1_single,exp2_single"
#eval_tags : "exp1_single"

extra_training: 0
extra_training_frac: 0.3
extra_training_tags: "exp4_single_ergast,exp8_single_stack,exp9_single_tpch"

dataset:
  input_train: ''
  input_test: ''
common:
  batch_size: 16
plan_net:
  arch: "gcn"
  hl: 64
  num_conv_layers: 8
  subplan_ests: 0
  dropout : 0.2
  pretrained: 0

sys_net:
  # mlp / transformer
  arch: "transformer"
  pretrained : 1
  save_weights: 0
  use_pretrained_norms: 1

  #pretrained_fn: "models4/noimdbwk_actual.wt"
  #pretrained_fn: "models4/noimdb_actual.wt"
  #pretrained_fn: "models4/tpch-ergast-stack_actual.wt"
  #pretrained_fn: "models4/mixed_noimdb_actual.wt"

  ## new ones
  #pretrained_fn: "models4/tpch-ergast-stack_log.wt"
  #pretrained_fn: "models4/mixed_noimdb_log.wt"
  #pretrained_fn: "models4/mixed_noimdb_log_nostatic.wt"
  #pretrained_fn: "models4/mixed_noimdb_log_nostatic_logfeats.wt"
  pretrained_fn: "models4/noactual_mixed_noimdb_logfeats_col.wt"

  max_pool: 0
  hl: 512
  num_layers: 4
  num_heads: 16
  log_prev_secs: 600
  log_skip: 2
  dropout: 0.0

factorized_net:
  arch: "mlp" # dot product
  embedding_size: 16
  pretrained: 1
  hl: 64
  num_layers: 2
  dropout: 0.2
  heuristic_feats: 0
...
