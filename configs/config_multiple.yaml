%YAML 1.2
---
#gpu: 0
traindata_dir : "LatencyCollectorResults/multiple"
#tags: "t7xlarge-gp3-c,t7xlarge-gp3-e-stressng,t7xlarge-gp3-d"
tags: "t7xlarge-gp3-a,t7xlarge-gp3-b,t7xlarge-gp3-e-stressng,t7xlarge-gp3-d"

use_eval_tags : 1
eval_dirs : "LatencyCollectorResults/multiple,LatencyCollectorResults"

#eval_tags: "t7xlarge-gp3-a,t7xlarge-gp3-b"
eval_tags: "t7xlarge-gp3-c"

#eval_tags: "CEB-c7large-gp2,CEB-t2medium-gp2,CEB-t2small-gp3,JOB-t2medium,JOB-t2micro,m5ad-gp2,CEB-c7xlarge-gp2,CEB-t2medium-gp3,JOB-c4x,JOB-t2medium-gp3,JOB-t2small"

common:
  batch_size: 16
plan_net:
  arch: "gcn"
  hl: 512
  num_conv_layers: 4
  subplan_ests: 0
  dropout: 0.2
sys_net:
  arch: "transformer"
  #arch: "mlp"
  hl: 256
  num_layers: 4
  num_heads: 16
  log_prev_secs: 200
  log_skip: 2
factorized_net:
  #arch: "mlp" # dot product
  arch: "dot"
  hl: 256
  num_layers: 2
  embedding_size: 16
...
