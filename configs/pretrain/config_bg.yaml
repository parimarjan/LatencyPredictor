%YAML 1.2
---
split_kind : "query" # instances,query,...;
random_split: "random" # random,custom
num_instances : -1
seed : 1
ignore_node_feats: "Alias,Filter"

#max_set_len : 50000
#num_bins : 5000
#num_bins : 1000

max_set_len : 100
num_bins : 50

#gpu: 0
#traindata_dir : "LatencyCollectorResults/background,LatencyCollectorResults/new,LatencyCollectorResults/multiple,LatencyCollectorResults"
traindata_dir : "LatencyCollectorResults/background"

#tags: "exp2_stack,exp3_stack_multi1,exp5_tpch_stats_ergast"

## no imdb
tags: "exp11_multi4_stack,exp12_multi4_tpch_stats_ergast,exp2_stack,exp3_stack_multi1,exp5_tpch_stats_ergast,exp9_stack1,exp9_stack2,"

use_eval_tags : 0
eval_dirs : "LatencyCollectorResults/new"
eval_tags : "exp2_single"

dataset:
  input_train: ''
  input_test: ''
common:
  batch_size: 16

plan_net:
  arch: "gcn"
  hl: 512
  num_conv_layers: 8
  subplan_ests: 0
  dropout : 0.2
sys_net:
  # mlp / transformer
  arch: "transformer"
  pretrained : 0
  save_weights: 1
  #pretrained_fn: "models2/bg_imdball_attention.wt"
  #pretrained_fn: "models2/bg_imdball_mlp.wt"
  #pretrained_fn: "models2/single_noimdbwk_bg_stack_attention.wt"
  #pretrained_fn: "models2/bg_exp2_exp5.wt"
  #pretrained_fn: "models2/bg_debug2.wt"
  #pretrained_fn: "models2/allbg_debug.wt"
  #pretrained_fn: "models2/bg_stack_attention_fact4.wt"
  #pretrained_fn: "models2/bg_stack_attention_small.wt"
  #pretrained_fn: "models2/bg_stack_attention_larger.wt"
  #pretrained_fn: "models2/bg_stack_ceb2_attention.wt"
  #pretrained_fn: "models2/all_noimdbwk_attention.wt"
  #pretrained_fn: "models2/all_attention.wt"
  #pretrained_fn: "models2/all_attention_100bins.wt"

  #pretrained_fn: "models2/bg_multi4.wt"
  #pretrained_fn: "models2/bg_multi4_stack1p.wt"
  #pretrained_fn: "models2/bg_multi4_imdb2p.wt"

  #pretrained_fn: "models3/bg_noimdb_ae2.wt"

  ## new ones
  pretrained_fn: "models3/new_bg_noimdb_ae.wt"

  max_pool: 1
  hl: 512
  ## normal
  num_layers: 4
  num_heads: 16

  log_prev_secs: 600
  log_skip: 2
  dropout: 0.2

factorized_net:
  arch: "mlp" # dot product
  embedding_size: 128
  pretrained: 0
  hl: 256
  num_layers: 2
  dropout: 0.2
...
